{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNYxyiyyuHYl50iyVKSM/Rn",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/penguix0/container-detection/blob/main/main.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 368
        },
        "id": "XoVlHbLpBU9P",
        "outputId": "506ee680-2c26-4efc-cb73-ed171ff76ae9"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-081410243c5c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# Import the dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mcontainer_dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m# Setup logging\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'container_dataset'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "## Import libaries\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "# Helper libraries\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "\n",
        "# Import the dataset\n",
        "import container_dataset\n",
        "\n",
        "# Setup logging\n",
        "import logging\n",
        "logger = tf.get_logger()\n",
        "logger.setLevel(logging.ERROR)\n",
        "\n",
        "\n",
        "## Import the dataset\n",
        "filename = \"main.py\"\n",
        "data_path = str(__file__).replace(filename, \"\") + \"dataset\"\n",
        "\n",
        "## Load the dataset\n",
        "(train_dataset, val_dataset, test_dataset), metadata = tfds.load(\"container_dataset\", \n",
        "    split=[\"train[:80%]\", \"train[80%:]\", \"test\"],\n",
        "    with_info=True)\n",
        "\n",
        "## Print classes which the program can detect\n",
        "class_names = metadata.features['objects'][\"label\"].names\n",
        "print(\"Class names: {}\".format(class_names))\n",
        "\n",
        "## Print the number of examples in each set\n",
        "num_train_examples = metadata.splits['train'].num_examples\n",
        "num_test_examples = metadata.splits['test'].num_examples\n",
        "print(\"Number of training examples: {}\".format(num_train_examples))\n",
        "print(\"Number of test examples:     {}\".format(num_test_examples))\n",
        "\n",
        "IMG_WIDTH = 100 #pixels\n",
        "IMG_HEIGHT = 100 #pixels\n",
        "IMG_DEPTH = 3\n",
        "\n",
        "train_dataset_img = []\n",
        "train_dataset_objects = []\n",
        "## Prepare all images\n",
        "for i in iter(train_dataset):\n",
        "    img = i[\"image\"]\n",
        "    train_dataset_img.append(img)\n",
        "    objects = []\n",
        "    for feature in i[\"objects\"]:\n",
        "        objects.append(i[\"objects\"][feature])\n",
        "    train_dataset_objects.append(objects)\n",
        "\n",
        "test_dataset_img = []\n",
        "test_dataset_objects = []\n",
        "## Prepare all images\n",
        "for i in iter(test_dataset):\n",
        "    img = i[\"image\"]\n",
        "    test_dataset_img.append(img)\n",
        "    objects = []\n",
        "    for feature in i[\"objects\"]:\n",
        "        objects.append(i[\"objects\"][feature])\n",
        "\n",
        "    test_dataset_objects.append(objects)\n",
        "\n",
        "val_dataset_img = []\n",
        "val_dataset_objects = []\n",
        "## Prepare all images\n",
        "for i in iter(val_dataset):\n",
        "    img = i[\"image\"]\n",
        "    val_dataset_img.append(img)\n",
        "    objects = []\n",
        "    for feature in i[\"objects\"]:\n",
        "        objects.append(i[\"objects\"][feature])\n",
        "    val_dataset_objects.append(objects)\n",
        "\n",
        "train_image_generator      = ImageDataGenerator(rescale=1./255)  # Generator for our training data\n",
        "test_image_generator       = ImageDataGenerator(rescale=1./255)  # Generator for our test data\n",
        "validation_image_generator = ImageDataGenerator(rescale=1./255)  # Generator for our validation data\n",
        "\n",
        "\n",
        "BATCH_SIZE = 5 # Number of training examples to process before updating our models variables\n",
        "\n",
        "print (train_dataset_objects)\n",
        "\n",
        "\n",
        "train_data_gen = train_image_generator.flow(x=np.asarray(train_dataset_img).astype('float64'),\n",
        "                                            y=train_dataset_objects,\n",
        "                                            batch_size=BATCH_SIZE,\n",
        "                                            shuffle=True)\n",
        "\n",
        "test_data_gen = validation_image_generator.flow(x=np.asarray(test_dataset_img).astype('float64'),\n",
        "                                               y=test_dataset_objects,\n",
        "                                               batch_size=BATCH_SIZE,\n",
        "                                               shuffle=False)\n",
        "\n",
        "val_data_gen = validation_image_generator.flow(x=np.asarray(val_dataset_img).astype('float64'),\n",
        "                                               y=val_dataset_objects,\n",
        "                                               batch_size=BATCH_SIZE,\n",
        "                                               shuffle=False)\n",
        "\n",
        "\n",
        "model = tf.keras.models.Sequential([\n",
        "    tf.keras.layers.Conv2D(32, (3,3), activation='relu', input_shape=(IMG_WIDTH, IMG_HEIGHT, IMG_DEPTH)),\n",
        "    tf.keras.layers.MaxPooling2D(2, 2),\n",
        "\n",
        "    tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n",
        "    tf.keras.layers.MaxPooling2D(2,2),\n",
        "    \n",
        "    tf.keras.layers.Conv2D(128, (3,3), activation='relu'),\n",
        "    tf.keras.layers.MaxPooling2D(2,2),\n",
        "    \n",
        "    tf.keras.layers.Conv2D(128, (3,3), activation='relu'),\n",
        "    tf.keras.layers.MaxPooling2D(2,2),\n",
        "    \n",
        "    tf.keras.layers.Flatten(),\n",
        "    tf.keras.layers.Dense(512, activation='relu'),\n",
        "    tf.keras.layers.Dense(4)\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam',\n",
        "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "EPOCHS = 14\n",
        "\n",
        "history = model.fit(\n",
        "    tf.convert_to_tensor(train_data_gen),\n",
        "    steps_per_epoch=int(np.ceil(len(train_dataset_img) / float(BATCH_SIZE))),\n",
        "    epochs=EPOCHS,\n",
        ")"
      ]
    }
  ]
}