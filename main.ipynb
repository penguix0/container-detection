{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/penguix0/container-detection/blob/main/main.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8fOv8pjdBsYj"
      },
      "source": [
        "## Install dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wIyMivGzB0O4",
        "outputId": "84ea462e-96cc-4c07-bb30-60cddb42d083"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.7/dist-packages (2.9.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from tensorflow) (21.3)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.3.0)\n",
            "Requirement already satisfied: keras<2.10.0,>=2.9.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (2.9.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (14.0.6)\n",
            "Requirement already satisfied: flatbuffers<2,>=1.12 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.12)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.15.0)\n",
            "Requirement already satisfied: tensorflow-estimator<2.10.0,>=2.9.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (2.9.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.1.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.14.1)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.21.6)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.1.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (4.1.1)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (2.1.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from tensorflow) (57.4.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.50.0)\n",
            "Requirement already satisfied: protobuf<3.20,>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.19.6)\n",
            "Requirement already satisfied: tensorboard<2.10,>=2.9 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (2.9.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.27.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.7/dist-packages (from astunparse>=1.6.0->tensorflow) (0.38.3)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py>=2.9.0->tensorflow) (1.5.2)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.10,>=2.9->tensorflow) (2.23.0)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.10,>=2.9->tensorflow) (1.0.1)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.10,>=2.9->tensorflow) (0.6.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.10,>=2.9->tensorflow) (1.8.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.10,>=2.9->tensorflow) (3.4.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.10,>=2.9->tensorflow) (0.4.6)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.10,>=2.9->tensorflow) (2.14.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow) (0.2.8)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow) (5.2.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.10,>=2.9->tensorflow) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<2.10,>=2.9->tensorflow) (4.13.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.10,>=2.9->tensorflow) (3.10.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow) (0.4.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow) (2022.9.24)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow) (1.24.3)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.10,>=2.9->tensorflow) (3.2.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->tensorflow) (3.0.9)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.7/dist-packages (7.1.2)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: tensorflow-datasets in /usr/local/lib/python3.7/dist-packages (4.6.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets) (1.15.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets) (4.1.1)\n",
            "Requirement already satisfied: toml in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets) (0.10.2)\n",
            "Requirement already satisfied: tensorflow-metadata in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets) (1.10.0)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets) (2.1.0)\n",
            "Requirement already satisfied: etils[epath] in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets) (0.9.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets) (4.64.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets) (1.21.6)\n",
            "Requirement already satisfied: protobuf>=3.12.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets) (3.19.6)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets) (5.10.0)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets) (0.3.6)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets) (2.23.0)\n",
            "Requirement already satisfied: promise in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets) (2.3)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets) (1.3.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->tensorflow-datasets) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->tensorflow-datasets) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->tensorflow-datasets) (2022.9.24)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->tensorflow-datasets) (3.0.4)\n",
            "Requirement already satisfied: zipp in /usr/local/lib/python3.7/dist-packages (from etils[epath]->tensorflow-datasets) (3.10.0)\n",
            "Requirement already satisfied: googleapis-common-protos<2,>=1.52.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-metadata->tensorflow-datasets) (1.56.4)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (1.21.6)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (3.2.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (0.11.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (1.4.4)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (3.0.9)\n",
            "Requirement already satisfied: numpy>=1.11 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (1.21.6)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from kiwisolver>=1.0.1->matplotlib) (4.1.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib) (1.15.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install tensorflow\n",
        "!pip install pillow\n",
        "!pip install tensorflow-datasets\n",
        "!pip install numpy\n",
        "!pip install matplotlib"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yQUcXbv4Cinb"
      },
      "source": [
        "## Import libraries"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hKNLnCYdCmf6"
      },
      "source": [
        "Import all libraries needed and configure the dataset and logging"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "zIV1bjLACr1Z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0fd801e9-289c-4c05-8d83-add61546516c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "bash: cannot set terminal process group (73): Inappropriate ioctl for device\n",
            "bash: no job control in this shell\n",
            "\u001b[01;34m/content\u001b[00m# ls\n",
            "\u001b[0m\u001b[01;34m'~'\u001b[0m   \u001b[01;34mcontainer_dataset\u001b[0m   \u001b[01;34mdrive\u001b[0m   \u001b[01;34msample_data\u001b[0m\n",
            "\u001b[01;34m/content\u001b[00m# cd container_dataset\n",
            "\u001b[01;34m/content/container_dataset\u001b[00m# ls\n",
            "checksums.tsv         container_dataset_test.py  \u001b[0m\u001b[01;34m__pycache__\u001b[0m\n",
            "container_dataset.py  __init__.py\n",
            "\u001b[01;34m/content/container_dataset\u001b[00m# tfds build --register_checksums\n",
            "2022-11-12 14:48:06.150877: E tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
            "INFO[build.py]: Loading dataset  from path: /content/container_dataset/container_dataset.py\n",
            "2022-11-12 14:48:06.347380: W tensorflow/core/platform/cloud/google_auth_provider.cc:184] All attempts to get a Google authentication bearer token failed, returning an empty token. Retrieving token from files failed with \"NOT_FOUND: Could not locate the credentials file.\". Retrieving token from GCE failed with \"NOT_FOUND: Error executing an HTTP request: HTTP response code 404\".\n",
            "INFO[build.py]: download_and_prepare for dataset container_dataset/1.0.0...\n",
            "INFO[dataset_builder.py]: Generating dataset container_dataset (/root/tensorflow_datasets/container_dataset/1.0.0)\n",
            "\u001b[1mDownloading and preparing dataset Unknown size (download: Unknown size, generated: Unknown size, total: Unknown size) to /root/tensorflow_datasets/container_dataset/1.0.0...\u001b[0m\n",
            "Dl Completed...: 0 url [00:00, ? url/s]\n",
            "Dl Size...: 0 MiB [00:00, ? MiB/s]\u001b[A\n",
            "\n",
            "                                       \n",
            "\u001b[A\n",
            "\n",
            "\u001b[A\u001b[AINFO[download_manager.py]: Downloading https://github.com/penguix0/container-detection-dataset/archive/refs/heads/main.zip into /root/tensorflow_datasets/downloads/peng_cont-dete-data_arch_refs_head_main-kv0na3wGu_-ARaeu5_AeSLnMNo1mP1hmN0eOsUl8hU.zip.tmp.4867613aab154d66bacf11e5ec3f59cd...\n",
            "Dl Completed...: 0 url [00:00, ? url/s]\n",
            "Dl Size...: 0 MiB [00:00, ? MiB/s]\u001b[A\n",
            "\n",
            "Dl Completed...:   0% 0/1 [00:00<?, ? url/s]\n",
            "Dl Size...: 0 MiB [00:00, ? MiB/s]\u001b[A\n",
            "\n",
            "Dl Completed...:   0% 0/1 [00:00<?, ? url/s]\n",
            "Dl Size...: 0 MiB [00:00, ? MiB/s]\u001b[A\n",
            "\n",
            "Dl Completed...: 100% 1/1 [00:00<00:00,  4.40 url/s]\n",
            "Dl Size...: 0 MiB [00:00, ? MiB/s]\u001b[A\n",
            "\n",
            "Dl Completed...: 100% 1/1 [00:00<00:00,  4.40 url/s]\n",
            "Dl Size...: 0 MiB [00:00, ? MiB/s]\u001b[A\n",
            "\n",
            "Extraction completed...:   0% 0/1 [00:00<?, ? file/s]\u001b[A\u001b[A\n",
            "\n",
            "Dl Completed...: 100% 1/1 [00:00<00:00,  4.40 url/s]\n",
            "Dl Size...: 0 MiB [00:00, ? MiB/s]\u001b[A\n",
            "\n",
            "Extraction completed...: 100% 1/1 [00:00<00:00,  3.40 file/s]\n",
            "Dl Size...: 0 MiB [00:00, ? MiB/s]\n",
            "Dl Completed...: 100% 1/1 [00:00<00:00,  3.39 url/s]\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/tfds\", line 8, in <module>\n",
            "    sys.exit(launch_cli())\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_datasets/scripts/cli/main.py\", line 102, in launch_cli\n",
            "    app.run(main, flags_parser=_parse_flags)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/absl/app.py\", line 308, in run\n",
            "    _run_main(main, args)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/absl/app.py\", line 254, in _run_main\n",
            "    sys.exit(main(argv))\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_datasets/scripts/cli/main.py\", line 97, in main\n",
            "    args.subparser_fn(args)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_datasets/scripts/cli/build.py\", line 192, in _build_datasets\n",
            "    _download_and_prepare(args, builder)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_datasets/scripts/cli/build.py\", line 344, in _download_and_prepare\n",
            "    download_config=dl_config,\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_datasets/core/dataset_builder.py\", line 483, in download_and_prepare\n",
            "    download_config=download_config,\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_datasets/core/dataset_builder.py\", line 1185, in _download_and_prepare\n",
            "    dl_manager, **optional_pipeline_kwargs)\n",
            "  File \"/content/container_dataset/container_dataset.py\", line 57, in _split_generators\n",
            "    dl_manager.register_checksums = True\n",
            "AttributeError: can't set attribute\n",
            "\u001b[01;34m/content/container_dataset\u001b[00m# \n",
            "\u001b[01;34m/content/container_dataset\u001b[00m# \n",
            "\u001b[01;34m/content/container_dataset\u001b[00m# tfds build --register_checksums\n",
            "2022-11-12 14:49:04.354342: E tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
            "INFO[build.py]: Loading dataset  from path: /content/container_dataset/container_dataset.py\n",
            "2022-11-12 14:49:04.452205: W tensorflow/core/platform/cloud/google_auth_provider.cc:184] All attempts to get a Google authentication bearer token failed, returning an empty token. Retrieving token from files failed with \"NOT_FOUND: Could not locate the credentials file.\". Retrieving token from GCE failed with \"NOT_FOUND: Error executing an HTTP request: HTTP response code 404\".\n",
            "INFO[build.py]: download_and_prepare for dataset container_dataset/1.0.0...\n",
            "INFO[dataset_builder.py]: Generating dataset container_dataset (/root/tensorflow_datasets/container_dataset/1.0.0)\n",
            "\u001b[1mDownloading and preparing dataset Unknown size (download: Unknown size, generated: Unknown size, total: Unknown size) to /root/tensorflow_datasets/container_dataset/1.0.0...\u001b[0m\n",
            "Dl Completed...: 0 url [00:00, ? url/s]\n",
            "Dl Size...: 0 MiB [00:00, ? MiB/s]\u001b[A\n",
            "\n",
            "                                       \n",
            "\u001b[A\n",
            "\n",
            "\u001b[A\u001b[AINFO[download_manager.py]: Skipping download of https://github.com/penguix0/container-detection-dataset/archive/refs/heads/main.zip: File cached in /root/tensorflow_datasets/downloads/peng_cont-dete-data_arch_refs_head_main7j0Cu9ihzf-cwhM1gtAigCioqafbKb1MLs9bUujaaTs.zip\n",
            "Dl Completed...: 0 url [00:00, ? url/s]\n",
            "Dl Size...: 0 MiB [00:00, ? MiB/s]\u001b[A\n",
            "\n",
            "Dl Completed...:   0% 0/1 [00:00<?, ? url/s]\n",
            "Dl Size...: 0 MiB [00:00, ? MiB/s]\u001b[A\n",
            "\n",
            "Dl Completed...: 100% 1/1 [00:00<00:00, 484.27 url/s]\n",
            "Dl Size...: 0 MiB [00:00, ? MiB/s]\u001b[A\n",
            "\n",
            "Dl Completed...: 100% 1/1 [00:00<00:00, 453.19 url/s]\n",
            "Dl Size...:   0% 0/945770 [00:00<?, ? MiB/s]\u001b[A\n",
            "\n",
            "Dl Completed...: 100% 1/1 [00:00<00:00, 428.91 url/s]\n",
            "Dl Size...: 100% 945770/945770 [00:00<00:00, 428941056.89 MiB/s]\u001b[A\n",
            "\n",
            "                                                     \n",
            "\u001b[A\n",
            "\n",
            "\u001b[A\u001b[AINFO[download_manager.py]: Reusing extraction of /root/tensorflow_datasets/downloads/peng_cont-dete-data_arch_refs_head_main7j0Cu9ihzf-cwhM1gtAigCioqafbKb1MLs9bUujaaTs.zip at /root/tensorflow_datasets/downloads/extracted/ZIP.peng_cont-dete-data_arch_refs_head_main7j0Cu9ihzf-cwhM1gtAigCioqafbKb1MLs9bUujaaTs.zip.\n",
            "Dl Completed...: 100% 1/1 [00:00<00:00, 268.87 url/s]\n",
            "Dl Size...: 100% 945770/945770 [00:00<00:00, 260599585.74 MiB/s]\u001b[A\n",
            "\n",
            "Extraction completed...: 0 file [00:00, ? file/s]\n",
            "Dl Size...: 100% 945770/945770 [00:00<00:00, 220466119.83 MiB/s]\n",
            "Dl Completed...: 100% 1/1 [00:00<00:00, 218.50 url/s]\n",
            "Generating splits...:   0% 0/2 [00:00<?, ? splits/s]\n",
            "Generating train examples...: 0 examples [00:00, ? examples/s]\u001b[A/root/tensorflow_datasets/downloads/extracted/ZIP.peng_cont-dete-data_arch_refs_head_main7j0Cu9ihzf-cwhM1gtAigCioqafbKb1MLs9bUujaaTs.zip/container-detection-dataset-main/training/IMG_007.json\n",
            "/root/tensorflow_datasets/downloads/extracted/ZIP.peng_cont-dete-data_arch_refs_head_main7j0Cu9ihzf-cwhM1gtAigCioqafbKb1MLs9bUujaaTs.zip/container-detection-dataset-main/training/IMG_027.json\n",
            "/root/tensorflow_datasets/downloads/extracted/ZIP.peng_cont-dete-data_arch_refs_head_main7j0Cu9ihzf-cwhM1gtAigCioqafbKb1MLs9bUujaaTs.zip/container-detection-dataset-main/training/IMG_006.json\n",
            "/root/tensorflow_datasets/downloads/extracted/ZIP.peng_cont-dete-data_arch_refs_head_main7j0Cu9ihzf-cwhM1gtAigCioqafbKb1MLs9bUujaaTs.zip/container-detection-dataset-main/training/IMG_063.json\n",
            "/root/tensorflow_datasets/downloads/extracted/ZIP.peng_cont-dete-data_arch_refs_head_main7j0Cu9ihzf-cwhM1gtAigCioqafbKb1MLs9bUujaaTs.zip/container-detection-dataset-main/training/IMG_084.json\n",
            "/root/tensorflow_datasets/downloads/extracted/ZIP.peng_cont-dete-data_arch_refs_head_main7j0Cu9ihzf-cwhM1gtAigCioqafbKb1MLs9bUujaaTs.zip/container-detection-dataset-main/training/IMG_003.json\n",
            "/root/tensorflow_datasets/downloads/extracted/ZIP.peng_cont-dete-data_arch_refs_head_main7j0Cu9ihzf-cwhM1gtAigCioqafbKb1MLs9bUujaaTs.zip/container-detection-dataset-main/training/IMG_057.json\n",
            "/root/tensorflow_datasets/downloads/extracted/ZIP.peng_cont-dete-data_arch_refs_head_main7j0Cu9ihzf-cwhM1gtAigCioqafbKb1MLs9bUujaaTs.zip/container-detection-dataset-main/training/IMG_050.json\n",
            "/root/tensorflow_datasets/downloads/extracted/ZIP.peng_cont-dete-data_arch_refs_head_main7j0Cu9ihzf-cwhM1gtAigCioqafbKb1MLs9bUujaaTs.zip/container-detection-dataset-main/training/IMG_013.json\n",
            "/root/tensorflow_datasets/downloads/extracted/ZIP.peng_cont-dete-data_arch_refs_head_main7j0Cu9ihzf-cwhM1gtAigCioqafbKb1MLs9bUujaaTs.zip/container-detection-dataset-main/training/IMG_028.json\n",
            "/root/tensorflow_datasets/downloads/extracted/ZIP.peng_cont-dete-data_arch_refs_head_main7j0Cu9ihzf-cwhM1gtAigCioqafbKb1MLs9bUujaaTs.zip/container-detection-dataset-main/training/IMG_065.json\n",
            "/root/tensorflow_datasets/downloads/extracted/ZIP.peng_cont-dete-data_arch_refs_head_main7j0Cu9ihzf-cwhM1gtAigCioqafbKb1MLs9bUujaaTs.zip/container-detection-dataset-main/training/IMG_048.json\n",
            "/root/tensorflow_datasets/downloads/extracted/ZIP.peng_cont-dete-data_arch_refs_head_main7j0Cu9ihzf-cwhM1gtAigCioqafbKb1MLs9bUujaaTs.zip/container-detection-dataset-main/training/IMG_017.json\n",
            "/root/tensorflow_datasets/downloads/extracted/ZIP.peng_cont-dete-data_arch_refs_head_main7j0Cu9ihzf-cwhM1gtAigCioqafbKb1MLs9bUujaaTs.zip/container-detection-dataset-main/training/IMG_062.json\n",
            "/root/tensorflow_datasets/downloads/extracted/ZIP.peng_cont-dete-data_arch_refs_head_main7j0Cu9ihzf-cwhM1gtAigCioqafbKb1MLs9bUujaaTs.zip/container-detection-dataset-main/training/IMG_054.json\n",
            "/root/tensorflow_datasets/downloads/extracted/ZIP.peng_cont-dete-data_arch_refs_head_main7j0Cu9ihzf-cwhM1gtAigCioqafbKb1MLs9bUujaaTs.zip/container-detection-dataset-main/training/IMG_009.json\n",
            "/root/tensorflow_datasets/downloads/extracted/ZIP.peng_cont-dete-data_arch_refs_head_main7j0Cu9ihzf-cwhM1gtAigCioqafbKb1MLs9bUujaaTs.zip/container-detection-dataset-main/training/IMG_060.json\n",
            "/root/tensorflow_datasets/downloads/extracted/ZIP.peng_cont-dete-data_arch_refs_head_main7j0Cu9ihzf-cwhM1gtAigCioqafbKb1MLs9bUujaaTs.zip/container-detection-dataset-main/training/IMG_086.json\n",
            "/root/tensorflow_datasets/downloads/extracted/ZIP.peng_cont-dete-data_arch_refs_head_main7j0Cu9ihzf-cwhM1gtAigCioqafbKb1MLs9bUujaaTs.zip/container-detection-dataset-main/training/IMG_023.json\n",
            "/root/tensorflow_datasets/downloads/extracted/ZIP.peng_cont-dete-data_arch_refs_head_main7j0Cu9ihzf-cwhM1gtAigCioqafbKb1MLs9bUujaaTs.zip/container-detection-dataset-main/training/IMG_037.json\n",
            "/root/tensorflow_datasets/downloads/extracted/ZIP.peng_cont-dete-data_arch_refs_head_main7j0Cu9ihzf-cwhM1gtAigCioqafbKb1MLs9bUujaaTs.zip/container-detection-dataset-main/training/IMG_035.json\n",
            "/root/tensorflow_datasets/downloads/extracted/ZIP.peng_cont-dete-data_arch_refs_head_main7j0Cu9ihzf-cwhM1gtAigCioqafbKb1MLs9bUujaaTs.zip/container-detection-dataset-main/training/IMG_012.json\n",
            "/root/tensorflow_datasets/downloads/extracted/ZIP.peng_cont-dete-data_arch_refs_head_main7j0Cu9ihzf-cwhM1gtAigCioqafbKb1MLs9bUujaaTs.zip/container-detection-dataset-main/training/IMG_040.json\n",
            "/root/tensorflow_datasets/downloads/extracted/ZIP.peng_cont-dete-data_arch_refs_head_main7j0Cu9ihzf-cwhM1gtAigCioqafbKb1MLs9bUujaaTs.zip/container-detection-dataset-main/training/IMG_029.json\n",
            "/root/tensorflow_datasets/downloads/extracted/ZIP.peng_cont-dete-data_arch_refs_head_main7j0Cu9ihzf-cwhM1gtAigCioqafbKb1MLs9bUujaaTs.zip/container-detection-dataset-main/training/IMG_042.json\n",
            "/root/tensorflow_datasets/downloads/extracted/ZIP.peng_cont-dete-data_arch_refs_head_main7j0Cu9ihzf-cwhM1gtAigCioqafbKb1MLs9bUujaaTs.zip/container-detection-dataset-main/training/IMG_078.json\n",
            "/root/tensorflow_datasets/downloads/extracted/ZIP.peng_cont-dete-data_arch_refs_head_main7j0Cu9ihzf-cwhM1gtAigCioqafbKb1MLs9bUujaaTs.zip/container-detection-dataset-main/training/IMG_077.json\n",
            "/root/tensorflow_datasets/downloads/extracted/ZIP.peng_cont-dete-data_arch_refs_head_main7j0Cu9ihzf-cwhM1gtAigCioqafbKb1MLs9bUujaaTs.zip/container-detection-dataset-main/training/IMG_045.json\n",
            "/root/tensorflow_datasets/downloads/extracted/ZIP.peng_cont-dete-data_arch_refs_head_main7j0Cu9ihzf-cwhM1gtAigCioqafbKb1MLs9bUujaaTs.zip/container-detection-dataset-main/training/IMG_049.json\n",
            "/root/tensorflow_datasets/downloads/extracted/ZIP.peng_cont-dete-data_arch_refs_head_main7j0Cu9ihzf-cwhM1gtAigCioqafbKb1MLs9bUujaaTs.zip/container-detection-dataset-main/training/IMG_044.json\n",
            "/root/tensorflow_datasets/downloads/extracted/ZIP.peng_cont-dete-data_arch_refs_head_main7j0Cu9ihzf-cwhM1gtAigCioqafbKb1MLs9bUujaaTs.zip/container-detection-dataset-main/training/IMG_051.json\n",
            "/root/tensorflow_datasets/downloads/extracted/ZIP.peng_cont-dete-data_arch_refs_head_main7j0Cu9ihzf-cwhM1gtAigCioqafbKb1MLs9bUujaaTs.zip/container-detection-dataset-main/training/IMG_024.json\n",
            "/root/tensorflow_datasets/downloads/extracted/ZIP.peng_cont-dete-data_arch_refs_head_main7j0Cu9ihzf-cwhM1gtAigCioqafbKb1MLs9bUujaaTs.zip/container-detection-dataset-main/training/IMG_066.json\n",
            "/root/tensorflow_datasets/downloads/extracted/ZIP.peng_cont-dete-data_arch_refs_head_main7j0Cu9ihzf-cwhM1gtAigCioqafbKb1MLs9bUujaaTs.zip/container-detection-dataset-main/training/IMG_018.json\n",
            "/root/tensorflow_datasets/downloads/extracted/ZIP.peng_cont-dete-data_arch_refs_head_main7j0Cu9ihzf-cwhM1gtAigCioqafbKb1MLs9bUujaaTs.zip/container-detection-dataset-main/training/IMG_079.json\n",
            "/root/tensorflow_datasets/downloads/extracted/ZIP.peng_cont-dete-data_arch_refs_head_main7j0Cu9ihzf-cwhM1gtAigCioqafbKb1MLs9bUujaaTs.zip/container-detection-dataset-main/training/IMG_016.json\n",
            "/root/tensorflow_datasets/downloads/extracted/ZIP.peng_cont-dete-data_arch_refs_head_main7j0Cu9ihzf-cwhM1gtAigCioqafbKb1MLs9bUujaaTs.zip/container-detection-dataset-main/training/IMG_026.json\n",
            "/root/tensorflow_datasets/downloads/extracted/ZIP.peng_cont-dete-data_arch_refs_head_main7j0Cu9ihzf-cwhM1gtAigCioqafbKb1MLs9bUujaaTs.zip/container-detection-dataset-main/training/IMG_038.json\n",
            "/root/tensorflow_datasets/downloads/extracted/ZIP.peng_cont-dete-data_arch_refs_head_main7j0Cu9ihzf-cwhM1gtAigCioqafbKb1MLs9bUujaaTs.zip/container-detection-dataset-main/training/IMG_021.json\n",
            "/root/tensorflow_datasets/downloads/extracted/ZIP.peng_cont-dete-data_arch_refs_head_main7j0Cu9ihzf-cwhM1gtAigCioqafbKb1MLs9bUujaaTs.zip/container-detection-dataset-main/training/IMG_032.json\n",
            "/root/tensorflow_datasets/downloads/extracted/ZIP.peng_cont-dete-data_arch_refs_head_main7j0Cu9ihzf-cwhM1gtAigCioqafbKb1MLs9bUujaaTs.zip/container-detection-dataset-main/training/IMG_055.json\n",
            "/root/tensorflow_datasets/downloads/extracted/ZIP.peng_cont-dete-data_arch_refs_head_main7j0Cu9ihzf-cwhM1gtAigCioqafbKb1MLs9bUujaaTs.zip/container-detection-dataset-main/training/IMG_081.json\n",
            "/root/tensorflow_datasets/downloads/extracted/ZIP.peng_cont-dete-data_arch_refs_head_main7j0Cu9ihzf-cwhM1gtAigCioqafbKb1MLs9bUujaaTs.zip/container-detection-dataset-main/training/IMG_030.json\n",
            "/root/tensorflow_datasets/downloads/extracted/ZIP.peng_cont-dete-data_arch_refs_head_main7j0Cu9ihzf-cwhM1gtAigCioqafbKb1MLs9bUujaaTs.zip/container-detection-dataset-main/training/IMG_061.json\n",
            "/root/tensorflow_datasets/downloads/extracted/ZIP.peng_cont-dete-data_arch_refs_head_main7j0Cu9ihzf-cwhM1gtAigCioqafbKb1MLs9bUujaaTs.zip/container-detection-dataset-main/training/IMG_047.json\n",
            "/root/tensorflow_datasets/downloads/extracted/ZIP.peng_cont-dete-data_arch_refs_head_main7j0Cu9ihzf-cwhM1gtAigCioqafbKb1MLs9bUujaaTs.zip/container-detection-dataset-main/training/IMG_070.json\n",
            "/root/tensorflow_datasets/downloads/extracted/ZIP.peng_cont-dete-data_arch_refs_head_main7j0Cu9ihzf-cwhM1gtAigCioqafbKb1MLs9bUujaaTs.zip/container-detection-dataset-main/training/IMG_033.json\n",
            "/root/tensorflow_datasets/downloads/extracted/ZIP.peng_cont-dete-data_arch_refs_head_main7j0Cu9ihzf-cwhM1gtAigCioqafbKb1MLs9bUujaaTs.zip/container-detection-dataset-main/training/IMG_015.json\n",
            "/root/tensorflow_datasets/downloads/extracted/ZIP.peng_cont-dete-data_arch_refs_head_main7j0Cu9ihzf-cwhM1gtAigCioqafbKb1MLs9bUujaaTs.zip/container-detection-dataset-main/training/IMG_053.json\n",
            "/root/tensorflow_datasets/downloads/extracted/ZIP.peng_cont-dete-data_arch_refs_head_main7j0Cu9ihzf-cwhM1gtAigCioqafbKb1MLs9bUujaaTs.zip/container-detection-dataset-main/training/IMG_010.json\n",
            "/root/tensorflow_datasets/downloads/extracted/ZIP.peng_cont-dete-data_arch_refs_head_main7j0Cu9ihzf-cwhM1gtAigCioqafbKb1MLs9bUujaaTs.zip/container-detection-dataset-main/training/IMG_041.json\n",
            "/root/tensorflow_datasets/downloads/extracted/ZIP.peng_cont-dete-data_arch_refs_head_main7j0Cu9ihzf-cwhM1gtAigCioqafbKb1MLs9bUujaaTs.zip/container-detection-dataset-main/training/IMG_059.json\n",
            "/root/tensorflow_datasets/downloads/extracted/ZIP.peng_cont-dete-data_arch_refs_head_main7j0Cu9ihzf-cwhM1gtAigCioqafbKb1MLs9bUujaaTs.zip/container-detection-dataset-main/training/IMG_064.json\n",
            "/root/tensorflow_datasets/downloads/extracted/ZIP.peng_cont-dete-data_arch_refs_head_main7j0Cu9ihzf-cwhM1gtAigCioqafbKb1MLs9bUujaaTs.zip/container-detection-dataset-main/training/IMG_039.json\n",
            "/root/tensorflow_datasets/downloads/extracted/ZIP.peng_cont-dete-data_arch_refs_head_main7j0Cu9ihzf-cwhM1gtAigCioqafbKb1MLs9bUujaaTs.zip/container-detection-dataset-main/training/IMG_052.json\n",
            "/root/tensorflow_datasets/downloads/extracted/ZIP.peng_cont-dete-data_arch_refs_head_main7j0Cu9ihzf-cwhM1gtAigCioqafbKb1MLs9bUujaaTs.zip/container-detection-dataset-main/training/IMG_046.json\n",
            "/root/tensorflow_datasets/downloads/extracted/ZIP.peng_cont-dete-data_arch_refs_head_main7j0Cu9ihzf-cwhM1gtAigCioqafbKb1MLs9bUujaaTs.zip/container-detection-dataset-main/training/IMG_022.json\n",
            "/root/tensorflow_datasets/downloads/extracted/ZIP.peng_cont-dete-data_arch_refs_head_main7j0Cu9ihzf-cwhM1gtAigCioqafbKb1MLs9bUujaaTs.zip/container-detection-dataset-main/training/IMG_034.json\n",
            "/root/tensorflow_datasets/downloads/extracted/ZIP.peng_cont-dete-data_arch_refs_head_main7j0Cu9ihzf-cwhM1gtAigCioqafbKb1MLs9bUujaaTs.zip/container-detection-dataset-main/training/IMG_005.json\n",
            "/root/tensorflow_datasets/downloads/extracted/ZIP.peng_cont-dete-data_arch_refs_head_main7j0Cu9ihzf-cwhM1gtAigCioqafbKb1MLs9bUujaaTs.zip/container-detection-dataset-main/training/IMG_074.json\n",
            "/root/tensorflow_datasets/downloads/extracted/ZIP.peng_cont-dete-data_arch_refs_head_main7j0Cu9ihzf-cwhM1gtAigCioqafbKb1MLs9bUujaaTs.zip/container-detection-dataset-main/training/IMG_083.json\n",
            "/root/tensorflow_datasets/downloads/extracted/ZIP.peng_cont-dete-data_arch_refs_head_main7j0Cu9ihzf-cwhM1gtAigCioqafbKb1MLs9bUujaaTs.zip/container-detection-dataset-main/training/IMG_019.json\n",
            "/root/tensorflow_datasets/downloads/extracted/ZIP.peng_cont-dete-data_arch_refs_head_main7j0Cu9ihzf-cwhM1gtAigCioqafbKb1MLs9bUujaaTs.zip/container-detection-dataset-main/training/IMG_067.json\n",
            "\n",
            "Generating train examples...: 63 examples [00:00, 627.19 examples/s]\u001b[A/root/tensorflow_datasets/downloads/extracted/ZIP.peng_cont-dete-data_arch_refs_head_main7j0Cu9ihzf-cwhM1gtAigCioqafbKb1MLs9bUujaaTs.zip/container-detection-dataset-main/training/IMG_020.json\n",
            "/root/tensorflow_datasets/downloads/extracted/ZIP.peng_cont-dete-data_arch_refs_head_main7j0Cu9ihzf-cwhM1gtAigCioqafbKb1MLs9bUujaaTs.zip/container-detection-dataset-main/training/IMG_011.json\n",
            "/root/tensorflow_datasets/downloads/extracted/ZIP.peng_cont-dete-data_arch_refs_head_main7j0Cu9ihzf-cwhM1gtAigCioqafbKb1MLs9bUujaaTs.zip/container-detection-dataset-main/training/IMG_001.json\n",
            "/root/tensorflow_datasets/downloads/extracted/ZIP.peng_cont-dete-data_arch_refs_head_main7j0Cu9ihzf-cwhM1gtAigCioqafbKb1MLs9bUujaaTs.zip/container-detection-dataset-main/training/IMG_069.json\n",
            "/root/tensorflow_datasets/downloads/extracted/ZIP.peng_cont-dete-data_arch_refs_head_main7j0Cu9ihzf-cwhM1gtAigCioqafbKb1MLs9bUujaaTs.zip/container-detection-dataset-main/training/IMG_068.json\n",
            "/root/tensorflow_datasets/downloads/extracted/ZIP.peng_cont-dete-data_arch_refs_head_main7j0Cu9ihzf-cwhM1gtAigCioqafbKb1MLs9bUujaaTs.zip/container-detection-dataset-main/training/IMG_075.json\n",
            "/root/tensorflow_datasets/downloads/extracted/ZIP.peng_cont-dete-data_arch_refs_head_main7j0Cu9ihzf-cwhM1gtAigCioqafbKb1MLs9bUujaaTs.zip/container-detection-dataset-main/training/IMG_073.json\n",
            "/root/tensorflow_datasets/downloads/extracted/ZIP.peng_cont-dete-data_arch_refs_head_main7j0Cu9ihzf-cwhM1gtAigCioqafbKb1MLs9bUujaaTs.zip/container-detection-dataset-main/training/IMG_085.json\n",
            "/root/tensorflow_datasets/downloads/extracted/ZIP.peng_cont-dete-data_arch_refs_head_main7j0Cu9ihzf-cwhM1gtAigCioqafbKb1MLs9bUujaaTs.zip/container-detection-dataset-main/training/IMG_058.json\n",
            "/root/tensorflow_datasets/downloads/extracted/ZIP.peng_cont-dete-data_arch_refs_head_main7j0Cu9ihzf-cwhM1gtAigCioqafbKb1MLs9bUujaaTs.zip/container-detection-dataset-main/training/IMG_043.json\n",
            "/root/tensorflow_datasets/downloads/extracted/ZIP.peng_cont-dete-data_arch_refs_head_main7j0Cu9ihzf-cwhM1gtAigCioqafbKb1MLs9bUujaaTs.zip/container-detection-dataset-main/training/IMG_082.json\n",
            "/root/tensorflow_datasets/downloads/extracted/ZIP.peng_cont-dete-data_arch_refs_head_main7j0Cu9ihzf-cwhM1gtAigCioqafbKb1MLs9bUujaaTs.zip/container-detection-dataset-main/training/IMG_072.json\n",
            "/root/tensorflow_datasets/downloads/extracted/ZIP.peng_cont-dete-data_arch_refs_head_main7j0Cu9ihzf-cwhM1gtAigCioqafbKb1MLs9bUujaaTs.zip/container-detection-dataset-main/training/IMG_076.json\n",
            "/root/tensorflow_datasets/downloads/extracted/ZIP.peng_cont-dete-data_arch_refs_head_main7j0Cu9ihzf-cwhM1gtAigCioqafbKb1MLs9bUujaaTs.zip/container-detection-dataset-main/training/IMG_002.json\n",
            "/root/tensorflow_datasets/downloads/extracted/ZIP.peng_cont-dete-data_arch_refs_head_main7j0Cu9ihzf-cwhM1gtAigCioqafbKb1MLs9bUujaaTs.zip/container-detection-dataset-main/training/IMG_080.json\n",
            "/root/tensorflow_datasets/downloads/extracted/ZIP.peng_cont-dete-data_arch_refs_head_main7j0Cu9ihzf-cwhM1gtAigCioqafbKb1MLs9bUujaaTs.zip/container-detection-dataset-main/training/IMG_014.json\n",
            "/root/tensorflow_datasets/downloads/extracted/ZIP.peng_cont-dete-data_arch_refs_head_main7j0Cu9ihzf-cwhM1gtAigCioqafbKb1MLs9bUujaaTs.zip/container-detection-dataset-main/training/IMG_031.json\n",
            "/root/tensorflow_datasets/downloads/extracted/ZIP.peng_cont-dete-data_arch_refs_head_main7j0Cu9ihzf-cwhM1gtAigCioqafbKb1MLs9bUujaaTs.zip/container-detection-dataset-main/training/IMG_071.json\n",
            "/root/tensorflow_datasets/downloads/extracted/ZIP.peng_cont-dete-data_arch_refs_head_main7j0Cu9ihzf-cwhM1gtAigCioqafbKb1MLs9bUujaaTs.zip/container-detection-dataset-main/training/IMG_008.json\n",
            "/root/tensorflow_datasets/downloads/extracted/ZIP.peng_cont-dete-data_arch_refs_head_main7j0Cu9ihzf-cwhM1gtAigCioqafbKb1MLs9bUujaaTs.zip/container-detection-dataset-main/training/IMG_036.json\n",
            "/root/tensorflow_datasets/downloads/extracted/ZIP.peng_cont-dete-data_arch_refs_head_main7j0Cu9ihzf-cwhM1gtAigCioqafbKb1MLs9bUujaaTs.zip/container-detection-dataset-main/training/IMG_056.json\n",
            "/root/tensorflow_datasets/downloads/extracted/ZIP.peng_cont-dete-data_arch_refs_head_main7j0Cu9ihzf-cwhM1gtAigCioqafbKb1MLs9bUujaaTs.zip/container-detection-dataset-main/training/IMG_025.json\n",
            "\n",
            "                                                                    \u001b[A\n",
            "Shuffling /root/tensorflow_datasets/container_dataset/1.0.0.incompleteMQFH5B/container_dataset-train.tfrecord*...:   0% 0/85 [00:00<?, ? examples/s]\u001b[A\n",
            "INFO[writer.py]: Done writing /root/tensorflow_datasets/container_dataset/1.0.0.incompleteMQFH5B/container_dataset-train.tfrecord*. Number of examples: 85 (shards: [85])\n",
            "Generating splits...:  50% 1/2 [00:00<00:00,  4.65 splits/s]\n",
            "Generating test examples...: 0 examples [00:00, ? examples/s]\u001b[A/root/tensorflow_datasets/downloads/extracted/ZIP.peng_cont-dete-data_arch_refs_head_main7j0Cu9ihzf-cwhM1gtAigCioqafbKb1MLs9bUujaaTs.zip/container-detection-dataset-main/testing/IMG_007.json\n",
            "/root/tensorflow_datasets/downloads/extracted/ZIP.peng_cont-dete-data_arch_refs_head_main7j0Cu9ihzf-cwhM1gtAigCioqafbKb1MLs9bUujaaTs.zip/container-detection-dataset-main/testing/IMG_006.json\n",
            "/root/tensorflow_datasets/downloads/extracted/ZIP.peng_cont-dete-data_arch_refs_head_main7j0Cu9ihzf-cwhM1gtAigCioqafbKb1MLs9bUujaaTs.zip/container-detection-dataset-main/testing/IMG_003.json\n",
            "/root/tensorflow_datasets/downloads/extracted/ZIP.peng_cont-dete-data_arch_refs_head_main7j0Cu9ihzf-cwhM1gtAigCioqafbKb1MLs9bUujaaTs.zip/container-detection-dataset-main/testing/IMG_009.json\n",
            "/root/tensorflow_datasets/downloads/extracted/ZIP.peng_cont-dete-data_arch_refs_head_main7j0Cu9ihzf-cwhM1gtAigCioqafbKb1MLs9bUujaaTs.zip/container-detection-dataset-main/testing/IMG_010.json\n",
            "/root/tensorflow_datasets/downloads/extracted/ZIP.peng_cont-dete-data_arch_refs_head_main7j0Cu9ihzf-cwhM1gtAigCioqafbKb1MLs9bUujaaTs.zip/container-detection-dataset-main/testing/IMG_005.json\n",
            "/root/tensorflow_datasets/downloads/extracted/ZIP.peng_cont-dete-data_arch_refs_head_main7j0Cu9ihzf-cwhM1gtAigCioqafbKb1MLs9bUujaaTs.zip/container-detection-dataset-main/testing/IMG_001.json\n",
            "/root/tensorflow_datasets/downloads/extracted/ZIP.peng_cont-dete-data_arch_refs_head_main7j0Cu9ihzf-cwhM1gtAigCioqafbKb1MLs9bUujaaTs.zip/container-detection-dataset-main/testing/IMG_002.json\n",
            "/root/tensorflow_datasets/downloads/extracted/ZIP.peng_cont-dete-data_arch_refs_head_main7j0Cu9ihzf-cwhM1gtAigCioqafbKb1MLs9bUujaaTs.zip/container-detection-dataset-main/testing/IMG_004.json\n",
            "/root/tensorflow_datasets/downloads/extracted/ZIP.peng_cont-dete-data_arch_refs_head_main7j0Cu9ihzf-cwhM1gtAigCioqafbKb1MLs9bUujaaTs.zip/container-detection-dataset-main/testing/IMG_008.json\n",
            "\n",
            "                                                             \u001b[A\n",
            "Shuffling /root/tensorflow_datasets/container_dataset/1.0.0.incompleteMQFH5B/container_dataset-test.tfrecord*...:   0% 0/10 [00:00<?, ? examples/s]\u001b[A\n",
            "INFO[writer.py]: Done writing /root/tensorflow_datasets/container_dataset/1.0.0.incompleteMQFH5B/container_dataset-test.tfrecord*. Number of examples: 10 (shards: [10])\n",
            "\u001b[1mDataset container_dataset downloaded and prepared to /root/tensorflow_datasets/container_dataset/1.0.0. Subsequent calls will reuse this data.\u001b[0m\n",
            "INFO[build.py]: Dataset generation complete...\n",
            "\n",
            "tfds.core.DatasetInfo(\n",
            "    name='container_dataset',\n",
            "    full_name='container_dataset/1.0.0',\n",
            "    description=\"\"\"\n",
            "    Description\n",
            "    \"\"\",\n",
            "    homepage='https://dataset-homepage/',\n",
            "    data_path='/root/tensorflow_datasets/container_dataset/1.0.0',\n",
            "    file_format=tfrecord,\n",
            "    download_size=923.60 KiB,\n",
            "    dataset_size=648.43 KiB,\n",
            "    features=FeaturesDict({\n",
            "        'image': Image(shape=(100, 100, 3), dtype=tf.uint8),\n",
            "        'objects': Sequence({\n",
            "            'label': ClassLabel(shape=(), dtype=tf.int64, num_classes=1),\n",
            "            'points': Tensor(shape=(None, 2), dtype=tf.float64),\n",
            "        }),\n",
            "    }),\n",
            "    supervised_keys=('image', 'objects'),\n",
            "    disable_shuffling=False,\n",
            "    splits={\n",
            "        'test': <SplitInfo num_examples=10, num_shards=1>,\n",
            "        'train': <SplitInfo num_examples=85, num_shards=1>,\n",
            "    },\n",
            "    citation=\"\"\"\"\"\",\n",
            ")\n",
            "\n",
            "\u001b[01;34m/content/container_dataset\u001b[00m# \n",
            "\u001b[01;34m/content/container_dataset\u001b[00m# exit\n"
          ]
        }
      ],
      "source": [
        "## Import libaries\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "# Helper libraries\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "\n",
        "# Import the dataset\n",
        "import container_dataset\n",
        "\n",
        "# Setup logging\n",
        "import logging\n",
        "logger = tf.get_logger()\n",
        "logger.setLevel(logging.ERROR)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4BnV-SjqDX8c"
      },
      "source": [
        "## Import and test the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qTLoSqlnDbW_",
        "outputId": "a46dcedf-712f-4304-b029-0f3e4ef38416"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Class names: ['container_front']\n",
            "Number of training examples: 85\n",
            "Number of test examples:     10\n"
          ]
        }
      ],
      "source": [
        "## Load the dataset\n",
        "(train_dataset, val_dataset, test_dataset), metadata = tfds.load(\"container_dataset\", \n",
        "    split=[\"train[:80%]\", \"train[80%:]\", \"test\"], \n",
        "    with_info=True)\n",
        "\n",
        "## Print classes which the program can detect\n",
        "class_names = metadata.features['objects'][\"label\"].names\n",
        "print(\"Class names: {}\".format(class_names))\n",
        "\n",
        "## Print the number of examples in each set\n",
        "num_train_examples = metadata.splits['train'].num_examples\n",
        "num_test_examples = metadata.splits['test'].num_examples\n",
        "print(\"Number of training examples: {}\".format(num_train_examples))\n",
        "print(\"Number of test examples:     {}\".format(num_test_examples))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Extract all data from datset"
      ],
      "metadata": {
        "id": "dIdJsPQ1UR5A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "IMG_WIDTH = 100 #pixels\n",
        "IMG_HEIGHT = 100 #pixels\n",
        "IMG_DEPTH = 3\n",
        "\n",
        "train_dataset_img = []\n",
        "train_dataset_objects = []\n",
        "## Prepare all images\n",
        "for i in iter(train_dataset):\n",
        "    img = i[\"image\"]\n",
        "    train_dataset_img.append(img)\n",
        "    objects = []\n",
        "    for feature in i[\"objects\"]:\n",
        "        objects.append(i[\"objects\"][feature])\n",
        "    train_dataset_objects.append(objects)\n",
        "\n",
        "test_dataset_img = []\n",
        "test_dataset_objects = []\n",
        "## Prepare all images\n",
        "for i in iter(test_dataset):\n",
        "    img = i[\"image\"]\n",
        "    test_dataset_img.append(img)\n",
        "    objects = []\n",
        "    for feature in i[\"objects\"]:\n",
        "        objects.append(i[\"objects\"][feature])\n",
        "\n",
        "    test_dataset_objects.append(objects)\n",
        "\n",
        "val_dataset_img = []\n",
        "val_dataset_objects = []\n",
        "## Prepare all images\n",
        "for i in iter(val_dataset):\n",
        "    img = i[\"image\"]\n",
        "    val_dataset_img.append(img)\n",
        "    objects = []\n",
        "    for feature in i[\"objects\"]:\n",
        "        objects.append(i[\"objects\"][feature])\n",
        "    val_dataset_objects.append(objects)\n",
        "\n",
        "train_image_generator      = ImageDataGenerator(rescale=1./255)  # Generator for our training data\n",
        "test_image_generator       = ImageDataGenerator(rescale=1./255)  # Generator for our test data\n",
        "validation_image_generator = ImageDataGenerator(rescale=1./255)  # Generator for our validation data\n",
        "\n"
      ],
      "metadata": {
        "id": "htPMLMIrUYng"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create flows from data"
      ],
      "metadata": {
        "id": "BkfOmr8uUfW2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 5 # Number of training examples to process before updating our models variables\n",
        "\n",
        "\n",
        "train_data_gen = train_image_generator.flow(x=np.asarray(train_dataset_img).astype('float64'),\n",
        "                                            y=train_dataset_objects,\n",
        "                                            batch_size=BATCH_SIZE,\n",
        "                                            shuffle=True)\n",
        "\n",
        "test_data_gen = validation_image_generator.flow(x=np.asarray(test_dataset_img).astype('float64'),\n",
        "                                               y=test_dataset_objects,\n",
        "                                               batch_size=BATCH_SIZE,\n",
        "                                               shuffle=False)\n",
        "\n",
        "val_data_gen = validation_image_generator.flow(x=np.asarray(val_dataset_img).astype('float64'),\n",
        "                                               y=val_dataset_objects,\n",
        "                                               batch_size=BATCH_SIZE,\n",
        "                                               shuffle=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d9cV-PUTUg1G",
        "outputId": "a0c04a5c-b615-41d7-841c-22b2bc5fff7b"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/preprocessing/image.py:689: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "  self.y = np.asarray(y)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create and compile a model"
      ],
      "metadata": {
        "id": "ZxInNUwUUinE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = tf.keras.models.Sequential([\n",
        "    tf.keras.layers.Conv2D(32, (3,3), activation='relu', input_shape=(IMG_WIDTH, IMG_HEIGHT, IMG_DEPTH)),\n",
        "    tf.keras.layers.MaxPooling2D(2, 2),\n",
        "\n",
        "    tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n",
        "    tf.keras.layers.MaxPooling2D(2,2),\n",
        "    \n",
        "    tf.keras.layers.Conv2D(128, (3,3), activation='relu'),\n",
        "    tf.keras.layers.MaxPooling2D(2,2),\n",
        "    \n",
        "    tf.keras.layers.Conv2D(128, (3,3), activation='relu'),\n",
        "    tf.keras.layers.MaxPooling2D(2,2),\n",
        "    \n",
        "    tf.keras.layers.Flatten(),\n",
        "    tf.keras.layers.Dense(512, activation='relu'),\n",
        "    tf.keras.layers.Dense(4)\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam',\n",
        "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "              metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "cNHeV43QUmCY"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train the model"
      ],
      "metadata": {
        "id": "OO0L1OKcUob3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCHS = 14\n",
        "\n",
        "history = model.fit(\n",
        "    tf.convert_to_tensor(train_data_gen),\n",
        "    steps_per_epoch=int(np.ceil(len(train_dataset_img) / float(BATCH_SIZE))),\n",
        "    epochs=EPOCHS,\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 232
        },
        "id": "cOEof8nzUrMP",
        "outputId": "d4fdffdb-ccf1-4de9-9af8-fb5077b031cd"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-33af2b593559>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mEPOCHS\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m14\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m history = model.fit(\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data_gen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mceil\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset_img\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "provenance": [],
      "mount_file_id": "1PBZRWartoVj-34PCTHuZjh_6es29tHDJ",
      "authorship_tag": "ABX9TyNrCzDLstta+9Tirw3uyOPR",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}