{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/penguix0/container-detection/blob/main/main.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8fOv8pjdBsYj"
      },
      "source": [
        "## Install dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wIyMivGzB0O4",
        "outputId": "a5e1109c-3310-4ce9-a558-05e8cd1d448f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.7/dist-packages (2.9.2)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (14.0.6)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (2.1.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.27.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (4.1.1)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from tensorflow) (21.3)\n",
            "Requirement already satisfied: flatbuffers<2,>=1.12 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.12)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: protobuf<3.20,>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.19.6)\n",
            "Requirement already satisfied: tensorboard<2.10,>=2.9 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (2.9.1)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.15.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.1.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from tensorflow) (57.4.0)\n",
            "Requirement already satisfied: tensorflow-estimator<2.10.0,>=2.9.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (2.9.0)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.4.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.1.2)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.50.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.3.0)\n",
            "Requirement already satisfied: keras<2.10.0,>=2.9.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (2.9.0)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.21.6)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.7/dist-packages (from astunparse>=1.6.0->tensorflow) (0.38.3)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py>=2.9.0->tensorflow) (1.5.2)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.10,>=2.9->tensorflow) (2.14.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.10,>=2.9->tensorflow) (1.8.1)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.10,>=2.9->tensorflow) (0.6.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.10,>=2.9->tensorflow) (1.0.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.10,>=2.9->tensorflow) (0.4.6)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.10,>=2.9->tensorflow) (3.4.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.10,>=2.9->tensorflow) (2.23.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow) (5.2.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow) (4.9)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow) (0.2.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.10,>=2.9->tensorflow) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<2.10,>=2.9->tensorflow) (4.13.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.10,>=2.9->tensorflow) (3.10.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow) (0.4.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow) (2022.9.24)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow) (1.24.3)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.10,>=2.9->tensorflow) (3.2.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->tensorflow) (3.0.9)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.7/dist-packages (7.1.2)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: tensorflow-datasets in /usr/local/lib/python3.7/dist-packages (4.6.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets) (4.64.1)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets) (0.3.6)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets) (1.21.6)\n",
            "Requirement already satisfied: etils[epath] in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets) (0.9.0)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets) (5.10.0)\n",
            "Requirement already satisfied: tensorflow-metadata in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets) (1.10.0)\n",
            "Requirement already satisfied: toml in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets) (0.10.2)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets) (1.3.0)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets) (2.23.0)\n",
            "Requirement already satisfied: protobuf>=3.12.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets) (3.19.6)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets) (4.1.1)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets) (2.1.0)\n",
            "Requirement already satisfied: promise in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets) (2.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets) (1.15.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->tensorflow-datasets) (2022.9.24)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->tensorflow-datasets) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->tensorflow-datasets) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->tensorflow-datasets) (2.10)\n",
            "Requirement already satisfied: zipp in /usr/local/lib/python3.7/dist-packages (from etils[epath]->tensorflow-datasets) (3.10.0)\n",
            "Requirement already satisfied: googleapis-common-protos<2,>=1.52.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-metadata->tensorflow-datasets) (1.56.4)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (1.21.6)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (3.2.2)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (3.0.9)\n",
            "Requirement already satisfied: numpy>=1.11 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (1.21.6)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (0.11.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (1.4.4)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from kiwisolver>=1.0.1->matplotlib) (4.1.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib) (1.15.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install tensorflow\n",
        "!pip install pillow\n",
        "!pip install tensorflow-datasets\n",
        "!pip install numpy\n",
        "!pip install matplotlib"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "```\n",
        "# Dit is opgemaakt als code\n",
        "```\n",
        "\n",
        "tfds build --register_checksums"
      ],
      "metadata": {
        "id": "pG38u8PgoKM5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!bash\n",
        "!cd container_dataset\n",
        "!tfds build --register_checksums"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KlSSa2byo-ff",
        "outputId": "7cdc7d57-ddbe-44f6-d27b-23029ffe01b9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "bash: cannot set terminal process group (79): Inappropriate ioctl for device\n",
            "bash: no job control in this shell\n",
            "\u001b[01;34m/content\u001b[00m# ls\n",
            "\u001b[0m\u001b[01;34mcontainer_dataset\u001b[0m  \u001b[01;34msample_data\u001b[0m\n",
            "\u001b[01;34m/content\u001b[00m# cd container_dataset\n",
            "\u001b[01;34m/content/container_dataset\u001b[00m# tfds build --register_hcecksums\n",
            "usage: tfds [-h] [--helpfull] [--version] {build,new} ...\n",
            "tfds: error: unrecognized arguments: --register_hcecksums\n",
            "\u001b[01;34m/content/container_dataset\u001b[00m# tfds build --register_checksums\n",
            "INFO[build.py]: Loading dataset  from path: /content/container_dataset/container_dataset.py\n",
            "2022-11-15 20:37:53.009886: W tensorflow/core/platform/cloud/google_auth_provider.cc:184] All attempts to get a Google authentication bearer token failed, returning an empty token. Retrieving token from files failed with \"NOT_FOUND: Could not locate the credentials file.\". Retrieving token from GCE failed with \"NOT_FOUND: Error executing an HTTP request: HTTP response code 404\".\n",
            "INFO[build.py]: download_and_prepare for dataset container_dataset/1.0.0...\n",
            "INFO[dataset_builder.py]: Generating dataset container_dataset (/root/tensorflow_datasets/container_dataset/1.0.0)\n",
            "\u001b[1mDownloading and preparing dataset Unknown size (download: Unknown size, generated: Unknown size, total: Unknown size) to /root/tensorflow_datasets/container_dataset/1.0.0...\u001b[0m\n",
            "Dl Completed...: 0 url [00:00, ? url/s]\n",
            "Dl Size...: 0 MiB [00:00, ? MiB/s]\u001b[A\n",
            "\n",
            "                                       \n",
            "\n",
            "\u001b[A\u001b[A\n",
            "\u001b[AINFO[download_manager.py]: Downloading https://github.com/penguix0/container-detection-dataset/archive/refs/heads/main.zip into /root/tensorflow_datasets/downloads/peng_cont-dete-data_arch_refs_head_main-kv0na3wGu_-ARaeu5_AeSLnMNo1mP1hmN0eOsUl8hU.zip.tmp.33bfd0d35fc14728945a5e74339e9c96...\n",
            "Dl Completed...: 0 url [00:00, ? url/s]\n",
            "\n",
            "Extraction completed...: 0 file [00:00, ? file/s]\u001b[A\u001b[A\n",
            "Dl Completed...:   0% 0/1 [00:00<?, ? url/s]\n",
            "Dl Size...: 0 MiB [00:00, ? MiB/s]\u001b[A\n",
            "\n",
            "Dl Completed...:   0% 0/1 [00:00<?, ? url/s]\n",
            "Dl Size...: 0 MiB [00:00, ? MiB/s]\u001b[A\n",
            "\n",
            "Dl Completed...: 100% 1/1 [00:00<00:00,  2.08 url/s]\n",
            "Dl Size...: 0 MiB [00:00, ? MiB/s]\u001b[A\n",
            "\n",
            "Dl Completed...: 100% 1/1 [00:00<00:00,  2.08 url/s]\n",
            "Dl Size...: 0 MiB [00:00, ? MiB/s]\u001b[A\n",
            "\n",
            "Extraction completed...:   0% 0/1 [00:00<?, ? file/s]\u001b[A\u001b[A\n",
            "\n",
            "Dl Completed...: 100% 1/1 [00:00<00:00,  2.08 url/s]\n",
            "Dl Size...: 0 MiB [00:00, ? MiB/s]\u001b[A\n",
            "\n",
            "Extraction completed...: 100% 1/1 [00:00<00:00,  1.88 file/s]\n",
            "Dl Size...: 0 MiB [00:00, ? MiB/s]\n",
            "Dl Completed...: 100% 1/1 [00:00<00:00,  1.88 url/s]\n",
            "Generating splits...:   0% 0/2 [00:00<?, ? splits/s]\n",
            "Generating train examples...: 0 examples [00:00, ? examples/s]\u001b[A\n",
            "                                                              \u001b[A\n",
            "Shuffling /root/tensorflow_datasets/container_dataset/1.0.0.incomplete7LR7DL/container_dataset-train.tfrecord*...:   0% 0/81 [00:00<?, ? examples/s]\u001b[A\n",
            "INFO[writer.py]: Done writing /root/tensorflow_datasets/container_dataset/1.0.0.incomplete7LR7DL/container_dataset-train.tfrecord*. Number of examples: 81 (shards: [81])\n",
            "Generating splits...:   0% 0/2 [00:00<?, ? splits/s]\n",
            "Generating test examples...: 0 examples [00:00, ? examples/s]\u001b[A\n",
            "                                                             \u001b[A\n",
            "Shuffling /root/tensorflow_datasets/container_dataset/1.0.0.incomplete7LR7DL/container_dataset-test.tfrecord*...:   0% 0/10 [00:00<?, ? examples/s]\u001b[A\n",
            "INFO[writer.py]: Done writing /root/tensorflow_datasets/container_dataset/1.0.0.incomplete7LR7DL/container_dataset-test.tfrecord*. Number of examples: 10 (shards: [10])\n",
            "\u001b[1mDataset container_dataset downloaded and prepared to /root/tensorflow_datasets/container_dataset/1.0.0. Subsequent calls will reuse this data.\u001b[0m\n",
            "INFO[build.py]: Dataset generation complete...\n",
            "\n",
            "tfds.core.DatasetInfo(\n",
            "    name='container_dataset',\n",
            "    full_name='container_dataset/1.0.0',\n",
            "    description=\"\"\"\n",
            "    Description\n",
            "    \"\"\",\n",
            "    homepage='https://dataset-homepage/',\n",
            "    data_path='/root/tensorflow_datasets/container_dataset/1.0.0',\n",
            "    file_format=tfrecord,\n",
            "    download_size=888.86 KiB,\n",
            "    dataset_size=619.82 KiB,\n",
            "    features=FeaturesDict({\n",
            "        'image': Image(shape=(100, 100, 3), dtype=tf.uint8),\n",
            "        'label': ClassLabel(shape=(), dtype=tf.int64, num_classes=2),\n",
            "        'points': Tensor(shape=(8,), dtype=tf.float32),\n",
            "    }),\n",
            "    supervised_keys=('image', ('label', 'points')),\n",
            "    disable_shuffling=False,\n",
            "    splits={\n",
            "        'test': <SplitInfo num_examples=10, num_shards=1>,\n",
            "        'train': <SplitInfo num_examples=81, num_shards=1>,\n",
            "    },\n",
            "    citation=\"\"\"\"\"\",\n",
            ")\n",
            "\n",
            "\u001b[01;34m/content/container_dataset\u001b[00m# exit\n",
            "exit\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/tfds\", line 8, in <module>\n",
            "    sys.exit(launch_cli())\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_datasets/scripts/cli/main.py\", line 102, in launch_cli\n",
            "    app.run(main, flags_parser=_parse_flags)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/absl/app.py\", line 308, in run\n",
            "    _run_main(main, args)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/absl/app.py\", line 254, in _run_main\n",
            "    sys.exit(main(argv))\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_datasets/scripts/cli/main.py\", line 97, in main\n",
            "    args.subparser_fn(args)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_datasets/scripts/cli/build.py\", line 191, in _build_datasets\n",
            "    for builder in _make_builders(args, ds_to_build):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_datasets/scripts/cli/build.py\", line 200, in _make_builders\n",
            "    builder_cls, builder_kwargs = _get_builder_cls(ds_to_build)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_datasets/scripts/cli/build.py\", line 249, in _get_builder_cls\n",
            "    path = _search_script_path(ds_to_build)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_datasets/scripts/cli/build.py\", line 287, in _search_script_path\n",
            "    return _validate_script_path(path / f'{path.name}.py')\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_datasets/scripts/cli/build.py\", line 313, in _validate_script_path\n",
            "    f'Could not find dataset generation script: {path}. ')\n",
            "FileNotFoundError: Could not find dataset generation script: /content/content.py. \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yQUcXbv4Cinb"
      },
      "source": [
        "## Import libraries"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hKNLnCYdCmf6"
      },
      "source": [
        "Import all libraries needed and configure the dataset and logging"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zIV1bjLACr1Z"
      },
      "outputs": [],
      "source": [
        "## Import libaries\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "# Helper libraries\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "\n",
        "# Import the dataset\n",
        "import container_dataset\n",
        "\n",
        "# Setup logging\n",
        "import logging\n",
        "logger = tf.get_logger()\n",
        "logger.setLevel(logging.ERROR)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4BnV-SjqDX8c"
      },
      "source": [
        "## Import and test the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qTLoSqlnDbW_",
        "outputId": "2ca3ba03-ea67-4f8d-c14e-1001b5dd19c0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Class names: ['none', 'container_front']\n",
            "Number of training examples: 81\n",
            "Number of test examples:     10\n"
          ]
        }
      ],
      "source": [
        "## Load the dataset\n",
        "(train_dataset, val_dataset, test_dataset), metadata = tfds.load(\"container_dataset\", \n",
        "    split=[\"train[:80%]\", \"train[80%:]\", \"test\"], \n",
        "    with_info=True)\n",
        "\n",
        "## Print classes which the program can detect\n",
        "class_names = metadata.features[\"label\"].names\n",
        "print(\"Class names: {}\".format(class_names))\n",
        "\n",
        "## Print the number of examples in each set\n",
        "num_train_examples = metadata.splits['train'].num_examples\n",
        "num_test_examples = metadata.splits['test'].num_examples\n",
        "print(\"Number of training examples: {}\".format(num_train_examples))\n",
        "print(\"Number of test examples:     {}\".format(num_test_examples))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Extract all data from datset"
      ],
      "metadata": {
        "id": "dIdJsPQ1UR5A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset_img = []\n",
        "train_dataset_label = []\n",
        "train_dataset_points = []\n",
        "## Put img, label and points into array\n",
        "for i in iter(train_dataset):\n",
        "    # Add the image to the array\n",
        "    img = i[\"image\"]\n",
        "    train_dataset_img.append(img)\n",
        "\n",
        "    label = i[\"label\"]\n",
        "    train_dataset_label.append(label)\n",
        "\n",
        "    points = i[\"points\"]\n",
        "    train_dataset_points.append(points)\n",
        "\n",
        "## Convert to numpy array\n",
        "train_dataset_img = np.array(train_dataset_img, dtype=\"float32\")\n",
        "train_dataset_label = np.array(train_dataset_label)\n",
        "train_dataset_points = np.array(train_dataset_points)\n",
        "\n",
        "\n",
        "test_dataset_img = []\n",
        "test_dataset_label = []\n",
        "test_dataset_points = []\n",
        "## Put img, label and points into array\n",
        "for i in iter(test_dataset):\n",
        "    img = i[\"image\"]\n",
        "    test_dataset_img.append(img)\n",
        "\n",
        "    label = i[\"label\"]\n",
        "    test_dataset_label.append(label)\n",
        "\n",
        "    points = i[\"points\"]\n",
        "    test_dataset_points.append(points)\n",
        "\n",
        "## Convert to numpy array\n",
        "test_dataset_img = np.array(test_dataset_img, dtype=\"float32\")\n",
        "test_dataset_label = np.array(test_dataset_label)\n",
        "test_dataset_points = np.array(test_dataset_points)\n",
        "\n",
        "val_dataset_img = []\n",
        "val_dataset_label = []\n",
        "val_dataset_points = []\n",
        "## Put img, label and points into array\n",
        "for i in iter(val_dataset):\n",
        "    img = i[\"image\"]\n",
        "    val_dataset_img.append(img)\n",
        " \n",
        "    label = i[\"label\"]\n",
        "    val_dataset_label.append(label)\n",
        "\n",
        "    points = i[\"points\"]\n",
        "    val_dataset_points.append(points)\n",
        "## Convert to numpy array\n",
        "val_dataset_img = np.array(val_dataset_img, dtype=\"float32\")\n",
        "val_dataset_label = np.array(val_dataset_label)\n",
        "val_dataset_points = np.array(val_dataset_points)"
      ],
      "metadata": {
        "id": "htPMLMIrUYng"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create the model input and base layers"
      ],
      "metadata": {
        "id": "BkfOmr8uUfW2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "IMG_WIDTH = 100 # pixels\n",
        "IMG_HEIGHT = 100 # pixels\n",
        "IMG_DEPTH = 3 # color channels\n",
        "\n",
        "#create the common input layer\n",
        "input_shape = (IMG_HEIGHT, IMG_WIDTH, IMG_DEPTH)\n",
        "input_layer = tf.keras.layers.Input(input_shape)\n",
        "\n",
        "#create the base layers\n",
        "base_layers = layers.experimental.preprocessing.Rescaling(1./255, name='bl_1')(input_layer)\n",
        "base_layers = layers.Conv2D(16, 3, padding='same', activation='relu', name='bl_2')(base_layers)\n",
        "base_layers = layers.MaxPooling2D(name='bl_3')(base_layers)\n",
        "base_layers = layers.Conv2D(32, 3, padding='same', activation='relu', name='bl_4')(base_layers)\n",
        "base_layers = layers.MaxPooling2D(name='bl_5')(base_layers)\n",
        "base_layers = layers.Conv2D(64, 3, padding='same', activation='relu', name='bl_6')(base_layers)\n",
        "base_layers = layers.MaxPooling2D(name='bl_7')(base_layers)\n",
        "base_layers = layers.Flatten(name='bl_8')(base_layers)\n"
      ],
      "metadata": {
        "id": "d9cV-PUTUg1G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create model classifier layers\n"
      ],
      "metadata": {
        "id": "EEjTnL9Lpt-C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#create the classifier branch\n",
        "classifier_branch = layers.Dense(128, activation='relu', name='cl_1')(base_layers)\n",
        "classifier_branch = layers.Dense(len(class_names), name='cl_head')(classifier_branch) "
      ],
      "metadata": {
        "id": "pZJewwhqpzo6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create model locator branch"
      ],
      "metadata": {
        "id": "VdvyM-dwqD2v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#create the localiser branch\n",
        "locator_branch = layers.Dense(128, activation='relu', name='bb_1')(base_layers)\n",
        "locator_branch = layers.Dense(64, activation='relu', name='bb_2')(locator_branch)\n",
        "locator_branch = layers.Dense(32, activation='relu', name='bb_3')(locator_branch)\n",
        "locator_branch = layers.Dense(8, activation='sigmoid', name='bb_head')(locator_branch)"
      ],
      "metadata": {
        "id": "CM-u2PPRqJ_V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create and compile a model"
      ],
      "metadata": {
        "id": "ZxInNUwUUinE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = tf.keras.Model(input_layer,\n",
        "           outputs=[classifier_branch,locator_branch])"
      ],
      "metadata": {
        "id": "cNHeV43QUmCY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since the two output branches were designed to achieve two different outcomes (one to output a probability distribution and the other to predict the actual bounding box values), it was necessary to set the appropriate loss functions to each branch. I used the Sparse Categorical Crossentropy loss function for the classification head and the Mean Squared Error (MSE) for the localiser head. I achieved this by defining the following dictionary."
      ],
      "metadata": {
        "id": "HKMnuqppqYnu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "losses = {\"cl_head\":tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), \n",
        "   \"bb_head\":tf.keras.losses.MSE}"
      ],
      "metadata": {
        "id": "4t-241zyqcbZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(loss=losses, optimizer='Adam', metrics=['accuracy'])\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fYSm5YvQqfed",
        "outputId": "6842da11-5f4b-4837-8410-3b3ab975bc51"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)           [(None, 100, 100, 3  0           []                               \n",
            "                                )]                                                                \n",
            "                                                                                                  \n",
            " bl_1 (Rescaling)               (None, 100, 100, 3)  0           ['input_1[0][0]']                \n",
            "                                                                                                  \n",
            " bl_2 (Conv2D)                  (None, 100, 100, 16  448         ['bl_1[0][0]']                   \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " bl_3 (MaxPooling2D)            (None, 50, 50, 16)   0           ['bl_2[0][0]']                   \n",
            "                                                                                                  \n",
            " bl_4 (Conv2D)                  (None, 50, 50, 32)   4640        ['bl_3[0][0]']                   \n",
            "                                                                                                  \n",
            " bl_5 (MaxPooling2D)            (None, 25, 25, 32)   0           ['bl_4[0][0]']                   \n",
            "                                                                                                  \n",
            " bl_6 (Conv2D)                  (None, 25, 25, 64)   18496       ['bl_5[0][0]']                   \n",
            "                                                                                                  \n",
            " bl_7 (MaxPooling2D)            (None, 12, 12, 64)   0           ['bl_6[0][0]']                   \n",
            "                                                                                                  \n",
            " bl_8 (Flatten)                 (None, 9216)         0           ['bl_7[0][0]']                   \n",
            "                                                                                                  \n",
            " bb_1 (Dense)                   (None, 128)          1179776     ['bl_8[0][0]']                   \n",
            "                                                                                                  \n",
            " bb_2 (Dense)                   (None, 64)           8256        ['bb_1[0][0]']                   \n",
            "                                                                                                  \n",
            " cl_1 (Dense)                   (None, 128)          1179776     ['bl_8[0][0]']                   \n",
            "                                                                                                  \n",
            " bb_3 (Dense)                   (None, 32)           2080        ['bb_2[0][0]']                   \n",
            "                                                                                                  \n",
            " cl_head (Dense)                (None, 2)            258         ['cl_1[0][0]']                   \n",
            "                                                                                                  \n",
            " bb_head (Dense)                (None, 8)            264         ['bb_3[0][0]']                   \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 2,393,994\n",
            "Trainable params: 2,393,994\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train the model"
      ],
      "metadata": {
        "id": "OO0L1OKcUob3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trainTargets = {\n",
        "    \"cl_head\": train_dataset_label,\n",
        "    \"bb_head\": train_dataset_points\n",
        "}\n",
        "validationTargets = {\n",
        "    \"cl_head\": val_dataset_label,\n",
        "    \"bb_head\": val_dataset_points\n",
        "}\n",
        "\n",
        "EPOCHS = 20\n",
        "history = model.fit(train_dataset_img, trainTargets,\n",
        "             validation_data=(val_dataset_img, validationTargets),\n",
        "             batch_size=4,\n",
        "             epochs=EPOCHS,\n",
        "             shuffle=True,\n",
        "             verbose=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cOEof8nzUrMP",
        "outputId": "87966928-9a9b-4a1c-8e26-9152d3fb5cca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "17/17 [==============================] - 0s 10ms/step - loss: 4091.4504 - cl_head_loss: 0.0000e+00 - bb_head_loss: 4091.4504 - cl_head_accuracy: 1.0000 - bb_head_accuracy: 0.0615 - val_loss: 4170.0166 - val_cl_head_loss: 0.0000e+00 - val_bb_head_loss: 4170.0166 - val_cl_head_accuracy: 1.0000 - val_bb_head_accuracy: 0.0000e+00\n",
            "Epoch 2/20\n",
            "17/17 [==============================] - 0s 8ms/step - loss: 4091.4509 - cl_head_loss: 0.0000e+00 - bb_head_loss: 4091.4509 - cl_head_accuracy: 1.0000 - bb_head_accuracy: 0.0615 - val_loss: 4170.0166 - val_cl_head_loss: 0.0000e+00 - val_bb_head_loss: 4170.0166 - val_cl_head_accuracy: 1.0000 - val_bb_head_accuracy: 0.0000e+00\n",
            "Epoch 3/20\n",
            "17/17 [==============================] - 0s 8ms/step - loss: 4091.4500 - cl_head_loss: 0.0000e+00 - bb_head_loss: 4091.4500 - cl_head_accuracy: 1.0000 - bb_head_accuracy: 0.0615 - val_loss: 4170.0166 - val_cl_head_loss: 0.0000e+00 - val_bb_head_loss: 4170.0166 - val_cl_head_accuracy: 1.0000 - val_bb_head_accuracy: 0.0000e+00\n",
            "Epoch 4/20\n",
            "17/17 [==============================] - 0s 8ms/step - loss: 4091.4504 - cl_head_loss: 0.0000e+00 - bb_head_loss: 4091.4504 - cl_head_accuracy: 1.0000 - bb_head_accuracy: 0.0615 - val_loss: 4170.0166 - val_cl_head_loss: 0.0000e+00 - val_bb_head_loss: 4170.0166 - val_cl_head_accuracy: 1.0000 - val_bb_head_accuracy: 0.0000e+00\n",
            "Epoch 5/20\n",
            "17/17 [==============================] - 0s 8ms/step - loss: 4091.4514 - cl_head_loss: 0.0000e+00 - bb_head_loss: 4091.4514 - cl_head_accuracy: 1.0000 - bb_head_accuracy: 0.0615 - val_loss: 4170.0166 - val_cl_head_loss: 0.0000e+00 - val_bb_head_loss: 4170.0166 - val_cl_head_accuracy: 1.0000 - val_bb_head_accuracy: 0.0000e+00\n",
            "Epoch 6/20\n",
            "17/17 [==============================] - 0s 8ms/step - loss: 4091.4504 - cl_head_loss: 0.0000e+00 - bb_head_loss: 4091.4504 - cl_head_accuracy: 1.0000 - bb_head_accuracy: 0.0615 - val_loss: 4170.0166 - val_cl_head_loss: 0.0000e+00 - val_bb_head_loss: 4170.0166 - val_cl_head_accuracy: 1.0000 - val_bb_head_accuracy: 0.0000e+00\n",
            "Epoch 7/20\n",
            "17/17 [==============================] - 0s 8ms/step - loss: 4091.4509 - cl_head_loss: 0.0000e+00 - bb_head_loss: 4091.4509 - cl_head_accuracy: 1.0000 - bb_head_accuracy: 0.0615 - val_loss: 4170.0166 - val_cl_head_loss: 0.0000e+00 - val_bb_head_loss: 4170.0166 - val_cl_head_accuracy: 1.0000 - val_bb_head_accuracy: 0.0000e+00\n",
            "Epoch 8/20\n",
            "17/17 [==============================] - 0s 9ms/step - loss: 4091.4504 - cl_head_loss: 0.0000e+00 - bb_head_loss: 4091.4504 - cl_head_accuracy: 1.0000 - bb_head_accuracy: 0.0615 - val_loss: 4170.0166 - val_cl_head_loss: 0.0000e+00 - val_bb_head_loss: 4170.0166 - val_cl_head_accuracy: 1.0000 - val_bb_head_accuracy: 0.0000e+00\n",
            "Epoch 9/20\n",
            "17/17 [==============================] - 0s 8ms/step - loss: 4091.4509 - cl_head_loss: 0.0000e+00 - bb_head_loss: 4091.4509 - cl_head_accuracy: 1.0000 - bb_head_accuracy: 0.0615 - val_loss: 4170.0166 - val_cl_head_loss: 0.0000e+00 - val_bb_head_loss: 4170.0166 - val_cl_head_accuracy: 1.0000 - val_bb_head_accuracy: 0.0000e+00\n",
            "Epoch 10/20\n",
            "17/17 [==============================] - 0s 8ms/step - loss: 4091.4504 - cl_head_loss: 0.0000e+00 - bb_head_loss: 4091.4504 - cl_head_accuracy: 1.0000 - bb_head_accuracy: 0.0615 - val_loss: 4170.0166 - val_cl_head_loss: 0.0000e+00 - val_bb_head_loss: 4170.0166 - val_cl_head_accuracy: 1.0000 - val_bb_head_accuracy: 0.0000e+00\n",
            "Epoch 11/20\n",
            "17/17 [==============================] - 0s 8ms/step - loss: 4091.4509 - cl_head_loss: 0.0000e+00 - bb_head_loss: 4091.4509 - cl_head_accuracy: 1.0000 - bb_head_accuracy: 0.0615 - val_loss: 4170.0166 - val_cl_head_loss: 0.0000e+00 - val_bb_head_loss: 4170.0166 - val_cl_head_accuracy: 1.0000 - val_bb_head_accuracy: 0.0000e+00\n",
            "Epoch 12/20\n",
            "17/17 [==============================] - 0s 8ms/step - loss: 4091.4509 - cl_head_loss: 0.0000e+00 - bb_head_loss: 4091.4509 - cl_head_accuracy: 1.0000 - bb_head_accuracy: 0.0615 - val_loss: 4170.0166 - val_cl_head_loss: 0.0000e+00 - val_bb_head_loss: 4170.0166 - val_cl_head_accuracy: 1.0000 - val_bb_head_accuracy: 0.0000e+00\n",
            "Epoch 13/20\n",
            "17/17 [==============================] - 0s 8ms/step - loss: 4091.4509 - cl_head_loss: 0.0000e+00 - bb_head_loss: 4091.4509 - cl_head_accuracy: 1.0000 - bb_head_accuracy: 0.0615 - val_loss: 4170.0166 - val_cl_head_loss: 0.0000e+00 - val_bb_head_loss: 4170.0166 - val_cl_head_accuracy: 1.0000 - val_bb_head_accuracy: 0.0000e+00\n",
            "Epoch 14/20\n",
            "17/17 [==============================] - 0s 8ms/step - loss: 4091.4504 - cl_head_loss: 0.0000e+00 - bb_head_loss: 4091.4504 - cl_head_accuracy: 1.0000 - bb_head_accuracy: 0.0615 - val_loss: 4170.0166 - val_cl_head_loss: 0.0000e+00 - val_bb_head_loss: 4170.0166 - val_cl_head_accuracy: 1.0000 - val_bb_head_accuracy: 0.0000e+00\n",
            "Epoch 15/20\n",
            "17/17 [==============================] - 0s 8ms/step - loss: 4091.4509 - cl_head_loss: 0.0000e+00 - bb_head_loss: 4091.4509 - cl_head_accuracy: 1.0000 - bb_head_accuracy: 0.0615 - val_loss: 4170.0166 - val_cl_head_loss: 0.0000e+00 - val_bb_head_loss: 4170.0166 - val_cl_head_accuracy: 1.0000 - val_bb_head_accuracy: 0.0000e+00\n",
            "Epoch 16/20\n",
            "17/17 [==============================] - 0s 8ms/step - loss: 4091.4504 - cl_head_loss: 0.0000e+00 - bb_head_loss: 4091.4504 - cl_head_accuracy: 1.0000 - bb_head_accuracy: 0.0615 - val_loss: 4170.0166 - val_cl_head_loss: 0.0000e+00 - val_bb_head_loss: 4170.0166 - val_cl_head_accuracy: 1.0000 - val_bb_head_accuracy: 0.0000e+00\n",
            "Epoch 17/20\n",
            "17/17 [==============================] - 0s 8ms/step - loss: 4091.4504 - cl_head_loss: 0.0000e+00 - bb_head_loss: 4091.4504 - cl_head_accuracy: 1.0000 - bb_head_accuracy: 0.0615 - val_loss: 4170.0166 - val_cl_head_loss: 0.0000e+00 - val_bb_head_loss: 4170.0166 - val_cl_head_accuracy: 1.0000 - val_bb_head_accuracy: 0.0000e+00\n",
            "Epoch 18/20\n",
            "17/17 [==============================] - 0s 8ms/step - loss: 4091.4509 - cl_head_loss: 0.0000e+00 - bb_head_loss: 4091.4509 - cl_head_accuracy: 1.0000 - bb_head_accuracy: 0.0615 - val_loss: 4170.0166 - val_cl_head_loss: 0.0000e+00 - val_bb_head_loss: 4170.0166 - val_cl_head_accuracy: 1.0000 - val_bb_head_accuracy: 0.0000e+00\n",
            "Epoch 19/20\n",
            "17/17 [==============================] - 0s 11ms/step - loss: 4091.4509 - cl_head_loss: 0.0000e+00 - bb_head_loss: 4091.4509 - cl_head_accuracy: 1.0000 - bb_head_accuracy: 0.0615 - val_loss: 4170.0166 - val_cl_head_loss: 0.0000e+00 - val_bb_head_loss: 4170.0166 - val_cl_head_accuracy: 1.0000 - val_bb_head_accuracy: 0.0000e+00\n",
            "Epoch 20/20\n",
            "17/17 [==============================] - 0s 8ms/step - loss: 4091.4504 - cl_head_loss: 0.0000e+00 - bb_head_loss: 4091.4504 - cl_head_accuracy: 1.0000 - bb_head_accuracy: 0.0615 - val_loss: 4170.0166 - val_cl_head_loss: 0.0000e+00 - val_bb_head_loss: 4170.0166 - val_cl_head_accuracy: 1.0000 - val_bb_head_accuracy: 0.0000e+00\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNRjBv24RkKU5h0eTAkWeSS",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}